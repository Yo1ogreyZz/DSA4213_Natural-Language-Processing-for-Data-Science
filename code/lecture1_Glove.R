#Install Required Packages fisrt
#For example, 
#install.packages("text2vec")

library(text2vec)

sentences <- c(
  "I love natural language processing",
  "Word embeddings capture semantic meanings",
  "GloVe embeddings are awesome",
  "We can learn word relationships",
  "I enjoy teaching NLP to students"
)

# Tokenize the sentences
tokens <- word_tokenizer(sentences)

# Create an iterator over the tokens
it <- itoken(tokens, progressbar = FALSE)

# Build the vocabulary
vocab <- create_vocabulary(it)

# Prune the vocabulary (optional: remove infrequent or frequent terms)
vocab <- prune_vocabulary(vocab, term_count_min = 1)

# Create a term-co-occurrence matrix (TCM)
tcm <- create_tcm(it, vectorizer = vocab_vectorizer(vocab), skip_grams_window = 5)

# Define the GloVe model
glove <- GlobalVectors$new(rank = 50, x_max = 10)  # rank = embedding dimensions

# Fit the GloVe model
word_vectors <- glove$fit_transform(tcm, n_iter = 20, convergence_tol = 0.01)

# Combine word and context embeddings (optional)
word_vectors <- word_vectors + t(glove$components)

# Retrieve embedding for a specific word
word <- "love"
word_embedding <- word_vectors[word, ]
print(word_embedding)

library(ggplot2)

# Perform PCA to reduce dimensions to 2D
pca <- prcomp(word_vectors, center = TRUE, scale. = TRUE)
word_vectors_pca <- data.frame(pca$x[, 1:2])
word_vectors_pca$word <- rownames(word_vectors)

# Plot the embeddings
ggplot(word_vectors_pca, aes(x = PC1, y = PC2, label = word)) +
  geom_point() +
  geom_text(aes(label = word), hjust = 0, vjust = 1, size = 3) +
  theme_minimal() +
  labs(title = "Word Embeddings Visualization", x = "PC1", y = "PC2")
